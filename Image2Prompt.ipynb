{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are we achieving image to prompts?\n",
    "\n",
    "Using the pretrained BLIP (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation)[https://huggingface.co/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration.forward.example] model from huggingface. Originally from LAVIS (Language-Vision Intelligence)[https://github.com/salesforce/LAVIS] by Salesforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b04690ac124f709e18b65ab37706ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a large circular hole in the ground\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "import keras\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", \n",
    ")\n",
    "model.to(device)\n",
    "url = \"/Users/rajathdb/Image2Prompts/sdip-images/images/20057f34d.png\"\n",
    "image = Image.open(url)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(**inputs)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the pipeline, we pass it to a question generator from the generated caption, which is used to further enhance the prompt by focusing on the potential region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1808783daebc452eab4043737dd3a631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    ")\n",
    "model.to(device)\n",
    "url = \"/Users/rajathdb/Image2Prompts/sdip-images/images/20057f34d.png\"\n",
    "image = Image.open(url)\n",
    "\n",
    "prompt = f\"Question: {generated_text}? Answer:\"\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device,)\n",
    "\n",
    "generated_ids = model.generate(**inputs)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "comp_path = 'sdip-images'\n",
    "imag_path = glob.glob(f'{comp_path}/images/*.png')\n",
    "images = os.listdir(os.path.join(comp_path, 'images'))\n",
    "imgIds = [i.split('.')[0] for i in images]\n",
    "\n",
    "EMBEDDING_LENGTH = 384\n",
    "eIds = list(range(EMBEDDING_LENGTH))\n",
    "\n",
    "imgId_eId = [\n",
    "    '_'.join(map(str, i)) for i in zip(\n",
    "        np.repeat(imgIds, EMBEDDING_LENGTH),\n",
    "        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sdip-images/images/d8edf2e40.png',\n",
       " 'sdip-images/images/a4e1c55a9.png',\n",
       " 'sdip-images/images/227ef0887.png',\n",
       " 'sdip-images/images/20057f34d.png',\n",
       " 'sdip-images/images/92e911621.png',\n",
       " 'sdip-images/images/f27825b2c.png',\n",
       " 'sdip-images/images/c98f79f71.png']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imag_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_png(image, 3)\n",
    "    image = tf.image.resize(image, (384, 384))\n",
    "    return image\n",
    "\n",
    "def tf_preprocess(images):\n",
    "    x = keras.layers.Rescaling(\n",
    "        scale=processor.image_processor.rescale_factor\n",
    "    )(images)\n",
    "    x = keras.layers.Normalization(\n",
    "        mean=processor.image_processor.image_mean,\n",
    "        variance=[x ** 2 for x in processor.image_processor.image_std],\n",
    "        axis=3\n",
    "    )(x)\n",
    "    x = keras.layers.Permute(\n",
    "        dims=(3, 1, 2)\n",
    "    )(x)\n",
    "    return x\n",
    "\n",
    "def dataloader(\n",
    "    image_paths, batch_size=1\n",
    "):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (image_paths)\n",
    "    )\n",
    "    dataset = dataset.map(read_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "    dataset = dataset.map(tf_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 06:08:37.129889: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-09 06:08:37.133349: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "test_ds = dataloader(\n",
    "    imag_path, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=TensorSpec(shape=(None, 3, 384, 384), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "st_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_ds):\n",
    "\n",
    "    global pred_prompts\n",
    "    submissions = []\n",
    "\n",
    "    for i, batch in enumerate(test_ds):\n",
    "        inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(**inputs)\n",
    "        pred_prompts = processor.batch_decode(output, skip_special_tokens=True)\n",
    "        \n",
    "        submissions.append(\n",
    "            model.encode(pred_prompts)\n",
    "        )\n",
    "    \n",
    "    return submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 prompt_embeddings = predict(test_ds)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 │   </span>submissions = []                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i, batch <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(test_ds):                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 7 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inputs = processor(images=batch, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>).to(device)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 │   │   </span>output = model.generate(**inputs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 │   │   </span>pred_prompts = processor.batch_decode(output, skip_special_tokens=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/models/blip_2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">proces</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">sing_blip_2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">103</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> text_encoding                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">101 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">102 │   │   # add pixel_values</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>103 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>encoding_image_processor = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.image_processor(images, return_tensors=return_te   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> text <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">106 │   │   │   </span>text_encoding = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenizer(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">image_processing_uti</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ls.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">458</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">455 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">456 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, images, **kwargs) -&gt; BatchFeature:                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">457 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Preprocess an image or a batch of images.\"\"\"</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>458 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.preprocess(images, **kwargs)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">459 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">460 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">preprocess</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, images, **kwargs) -&gt; BatchFeature:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">461 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">NotImplementedError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Each image processor must implement its own preproces</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/models/blip/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">image_pr</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ocessing_blip.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">271</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">preprocess</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">268 │   │   </span>images = [to_numpy_array(image) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> image <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> images]                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">269 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">270 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> do_resize:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>271 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>images = [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.resize(image=image, size=size, resample=resample) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> image <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">i</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">272 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">273 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> do_rescale:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">274 │   │   │   </span>images = [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rescale(image=image, scale=rescale_factor) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> image <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> image   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/models/blip/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">image_pr</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ocessing_blip.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">271</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">268 │   │   </span>images = [to_numpy_array(image) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> image <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> images]                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">269 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">270 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> do_resize:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>271 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>images = [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.resize(image=image, size=size, resample=resample) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> image <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">i</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">272 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">273 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> do_rescale:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">274 │   │   │   </span>images = [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rescale(image=image, scale=rescale_factor) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> image <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> image   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/models/blip/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">image_pr</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ocessing_blip.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">134</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">resize</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">131 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">132 │   │   </span>size = get_size_dict(size, default_to_square=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">133 │   │   </span>output_size = (size[<span style=\"color: #808000; text-decoration-color: #808000\">\"width\"</span>], size[<span style=\"color: #808000; text-decoration-color: #808000\">\"height\"</span>])                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>134 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> resize(image, size=output_size, resample=resample, data_format=data_forma   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">135 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">136 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">rescale</span>(                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">image_transforms.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">295</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">resize</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">292 │   # To maintain backwards compatibility with the resizing done in previous image featu</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">293 │   # the pillow library to resize the image and then convert back to numpy</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">294 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(image, PIL.Image.Image):                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>295 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>image = to_pil_image(image)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">296 │   </span>height, width = size                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">297 │   # PIL images are in the format (width, height)</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">298 │   </span>resized_image = image.resize((width, height), resample=resample, reducing_gap=reduci   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">image_transforms.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">174</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">to_pil_image</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> np.all(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> &lt;= image) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> np.all(image &lt;= <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>):                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 │   │   │   </span>do_rescale = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">173 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>174 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"The image to be converted to a PIL image contains values outside the ra</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"got [{</span>image.min()<span style=\"color: #808000; text-decoration-color: #808000\">}, {</span>image.max()<span style=\"color: #808000; text-decoration-color: #808000\">}] which cannot be converted to uint8.</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">177 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>The image to be converted to a PIL image contains values outside the range <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, got \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.7922625541687012</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.1458969116210938</span><span style=\"font-weight: bold\">]</span> which cannot be converted to uint8.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 prompt_embeddings = predict(test_ds)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mpredict\u001b[0m:\u001b[94m7\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0msubmissions = []                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m i, batch \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(test_ds):                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 7 \u001b[2m│   │   \u001b[0minputs = processor(images=batch, return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m).to(device)                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   │   \u001b[0moutput = model.generate(**inputs)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   │   \u001b[0mpred_prompts = processor.batch_decode(output, skip_special_tokens=\u001b[94mTrue\u001b[0m)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/models/blip_2/\u001b[0m\u001b[1;33mproces\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33msing_blip_2.py\u001b[0m:\u001b[94m103\u001b[0m in \u001b[92m__call__\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m text_encoding                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m102 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# add pixel_values\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m103 \u001b[2m│   │   \u001b[0mencoding_image_processor = \u001b[96mself\u001b[0m.image_processor(images, return_tensors=return_te   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m text \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext_encoding = \u001b[96mself\u001b[0m.tokenizer(                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mimage_processing_uti\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mls.py\u001b[0m:\u001b[94m458\u001b[0m in \u001b[92m__call__\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m455 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m456 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__call__\u001b[0m(\u001b[96mself\u001b[0m, images, **kwargs) -> BatchFeature:                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m457 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m458 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.preprocess(images, **kwargs)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m459 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m460 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mpreprocess\u001b[0m(\u001b[96mself\u001b[0m, images, **kwargs) -> BatchFeature:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m461 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mNotImplementedError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mEach image processor must implement its own preproces\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/models/blip/\u001b[0m\u001b[1;33mimage_pr\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mocessing_blip.py\u001b[0m:\u001b[94m271\u001b[0m in \u001b[92mpreprocess\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m268 \u001b[0m\u001b[2m│   │   \u001b[0mimages = [to_numpy_array(image) \u001b[94mfor\u001b[0m image \u001b[95min\u001b[0m images]                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m269 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m270 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m do_resize:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m271 \u001b[2m│   │   │   \u001b[0mimages = [\u001b[96mself\u001b[0m.resize(image=image, size=size, resample=resample) \u001b[94mfor\u001b[0m image \u001b[95mi\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m272 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m273 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m do_rescale:                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m274 \u001b[0m\u001b[2m│   │   │   \u001b[0mimages = [\u001b[96mself\u001b[0m.rescale(image=image, scale=rescale_factor) \u001b[94mfor\u001b[0m image \u001b[95min\u001b[0m image   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/models/blip/\u001b[0m\u001b[1;33mimage_pr\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mocessing_blip.py\u001b[0m:\u001b[94m271\u001b[0m in \u001b[92m<listcomp>\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m268 \u001b[0m\u001b[2m│   │   \u001b[0mimages = [to_numpy_array(image) \u001b[94mfor\u001b[0m image \u001b[95min\u001b[0m images]                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m269 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m270 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m do_resize:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m271 \u001b[2m│   │   │   \u001b[0mimages = [\u001b[96mself\u001b[0m.resize(image=image, size=size, resample=resample) \u001b[94mfor\u001b[0m image \u001b[95mi\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m272 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m273 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m do_rescale:                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m274 \u001b[0m\u001b[2m│   │   │   \u001b[0mimages = [\u001b[96mself\u001b[0m.rescale(image=image, scale=rescale_factor) \u001b[94mfor\u001b[0m image \u001b[95min\u001b[0m image   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/models/blip/\u001b[0m\u001b[1;33mimage_pr\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mocessing_blip.py\u001b[0m:\u001b[94m134\u001b[0m in \u001b[92mresize\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m131 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m132 \u001b[0m\u001b[2m│   │   \u001b[0msize = get_size_dict(size, default_to_square=\u001b[94mTrue\u001b[0m)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m133 \u001b[0m\u001b[2m│   │   \u001b[0moutput_size = (size[\u001b[33m\"\u001b[0m\u001b[33mwidth\u001b[0m\u001b[33m\"\u001b[0m], size[\u001b[33m\"\u001b[0m\u001b[33mheight\u001b[0m\u001b[33m\"\u001b[0m])                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m134 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m resize(image, size=output_size, resample=resample, data_format=data_forma   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m136 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mrescale\u001b[0m(                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mimage_transforms.py\u001b[0m: \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m295\u001b[0m in \u001b[92mresize\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# To maintain backwards compatibility with the resizing done in previous image featu\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m293 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# the pillow library to resize the image and then convert back to numpy\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m294 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(image, PIL.Image.Image):                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m295 \u001b[2m│   │   \u001b[0mimage = to_pil_image(image)                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m296 \u001b[0m\u001b[2m│   \u001b[0mheight, width = size                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m297 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# PIL images are in the format (width, height)\u001b[0m                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m298 \u001b[0m\u001b[2m│   \u001b[0mresized_image = image.resize((width, height), resample=resample, reducing_gap=reduci   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/rajathdb/miniconda3/envs/rl/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mimage_transforms.py\u001b[0m: \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m174\u001b[0m in \u001b[92mto_pil_image\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m np.all(\u001b[94m0\u001b[0m <= image) \u001b[95mand\u001b[0m np.all(image <= \u001b[94m1\u001b[0m):                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   │   │   \u001b[0mdo_rescale = \u001b[94mTrue\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m173 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m174 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mThe image to be converted to a PIL image contains values outside the ra\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m176 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mgot [\u001b[0m\u001b[33m{\u001b[0mimage.min()\u001b[33m}\u001b[0m\u001b[33m, \u001b[0m\u001b[33m{\u001b[0mimage.max()\u001b[33m}\u001b[0m\u001b[33m] which cannot be converted to uint8.\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m177 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mThe image to be converted to a PIL image contains values outside the range \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, got \n",
       "\u001b[1m[\u001b[0m\u001b[1;36m-1.7922625541687012\u001b[0m, \u001b[1;36m2.1458969116210938\u001b[0m\u001b[1m]\u001b[0m which cannot be converted to uint8.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_embeddings = predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
